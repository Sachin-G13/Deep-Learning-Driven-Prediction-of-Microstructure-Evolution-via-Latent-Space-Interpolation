{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a14f392",
   "metadata": {},
   "source": [
    "### Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232cea2e-fcd1-4072-a224-edc610646b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, UpSampling2D, Flatten, Dense, Reshape, Lambda, Concatenate\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import skimage as sk\n",
    "from skimage import io, color, measure\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198e5de2",
   "metadata": {},
   "source": [
    "### Extracting the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ace5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "zip_path = \"dataset.zip\"\n",
    "extract_to = os.path.join(os.getcwd(), \"dataset\")\n",
    "\n",
    "# Create the target directory if it doesn't exist\n",
    "os.makedirs(extract_to, exist_ok=True)\n",
    "\n",
    "# Unzip the file\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_to)\n",
    "\n",
    "print(f\"Extracted '{zip_path}' to '{extract_to}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc75976a",
   "metadata": {},
   "source": [
    "### Loading the images and their corresponding labels from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b70f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_data, count_labels\n",
    "\n",
    "# Parameters\n",
    "data_dir = 'dataset/dataset' # Specify the path to your data directory\n",
    "img_height, img_width, img_channels = 256, 256, 1 # Input image dimensions\n",
    "num_classes = 9 # Number of classes\n",
    "\n",
    "composition_labels = {\n",
    "    27: 0,\n",
    "    31: 1,\n",
    "    35: 2,\n",
    "    39: 3,\n",
    "    40: 4,\n",
    "    42: 5,\n",
    "    44: 6,\n",
    "    46: 7,\n",
    "    48: 8\n",
    "}\n",
    "\n",
    "images, labels = load_data(data_dir, img_height, img_width, img_channels, num_classes, composition_labels)\n",
    "\n",
    "print(f\"{len(images)} images found\")\n",
    "print(f\"{len(labels)} labels found\")\n",
    "count_labels(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff564dc",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85b302b",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = (16, 16, 1) # Latent space dimensions\n",
    "\n",
    "class CVAE(tf.keras.Model):\n",
    "    def __init__(self, latent_dim, num_classes):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.encoder = self.build_encoder()\n",
    "        self.decoder = self.build_decoder()\n",
    "\n",
    "    def build_encoder(self):\n",
    "        input_img = Input(shape=(img_height, img_width, img_channels))\n",
    "        input_label = Input(shape=(self.num_classes,))\n",
    "        label_embedding = Dense(img_height * img_width)(input_label)\n",
    "        label_embedding = Reshape((img_height, img_width, 1))(label_embedding)\n",
    "\n",
    "        x = Concatenate()([input_img, label_embedding])\n",
    "        x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "        x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "        x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "        x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "        x = Flatten()(x)\n",
    "        mean = Dense(self.latent_dim[0] * self.latent_dim[1] * self.latent_dim[2])(x)\n",
    "        log_var = Dense(self.latent_dim[0] * self.latent_dim[1] * self.latent_dim[2])(x)\n",
    "        mean = Reshape(self.latent_dim)(mean)\n",
    "        log_var = Reshape(self.latent_dim)(log_var)\n",
    "        return Model([input_img, input_label], [mean, log_var], name='encoder')\n",
    "\n",
    "    def build_decoder(self):\n",
    "        latent_input = Input(shape=self.latent_dim)\n",
    "        input_label = Input(shape=(self.num_classes,))\n",
    "        label_embedding = Dense(self.latent_dim[0] * self.latent_dim[1] * self.latent_dim[2])(input_label)\n",
    "        label_embedding = Reshape(self.latent_dim)(label_embedding)\n",
    "\n",
    "        x = Concatenate()([latent_input, label_embedding])\n",
    "        x = Reshape((self.latent_dim[0] * self.latent_dim[1] * self.latent_dim[2] * 2,))(x)\n",
    "        x = Dense(16 * 16 * 512, activation='relu')(x)\n",
    "        x = Reshape((16, 16, 512))(x)\n",
    "        x = Conv2DTranspose(256, (3, 3), activation='relu', strides=2, padding='same')(x)\n",
    "        x = Conv2DTranspose(128, (3, 3), activation='relu', strides=2, padding='same')(x)\n",
    "        x = Conv2DTranspose(64, (3, 3), activation='relu', strides=2, padding='same')(x)\n",
    "        x = Conv2DTranspose(32, (3, 3), activation='relu', strides=2, padding='same')(x)\n",
    "        output_img = Conv2D(img_channels, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "        return Model([latent_input, input_label], output_img, name='decoder')\n",
    "\n",
    "    def sampling(self, args):\n",
    "        mean, log_var = args\n",
    "        epsilon = tf.random.normal(shape=tf.shape(mean), mean=0., stddev=1.)\n",
    "        return mean + tf.exp(log_var / 2) * epsilon\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_img, input_label = inputs\n",
    "        mean, log_var = self.encoder([input_img, input_label])\n",
    "        z = self.sampling([mean, log_var])\n",
    "        reconstructed = self.decoder([z, input_label])\n",
    "        reconstruction_loss = tf.reduce_mean(MeanSquaredError()(input_img, reconstructed))\n",
    "        kl_loss = -0.5 * tf.reduce_mean(1 + log_var - tf.square(mean) - tf.exp(log_var))\n",
    "        self.add_loss(reconstruction_loss + kl_loss)\n",
    "        return reconstructed\n",
    "\n",
    "cvae = CVAE(latent_dim, num_classes)\n",
    "cvae.compile(optimizer=Adam())\n",
    "\n",
    "cvae.encoder.summary()\n",
    "cvae.decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e791a2",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a857dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Adam optimizer with initial learning rate\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "# # Set up callbacks\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=30\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.85,  # Reducing lr by 5% when triggered\n",
    "    patience=5,   # Number of epochs with no improvement after which learning rate will be reduced\n",
    "    min_lr=1e-6,  # LR won't reduce below this\n",
    "    min_delta=1e-3  # Minimum change to count as an improvement\n",
    ")\n",
    "\n",
    "# Splitting the dataset into training and validation sets\n",
    "labels_integer = np.argmax(labels, axis=1)\n",
    "train_images, val_images, train_labels, val_labels = train_test_split(\n",
    "    images, labels, \n",
    "    test_size=0.2, \n",
    "    stratify=labels_integer\n",
    ")\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Training with both the callbacks\n",
    "cvae.fit(\n",
    "    [train_images, train_labels],\n",
    "    train_images,\n",
    "    epochs=500,\n",
    "    batch_size=16,\n",
    "    validation_data=([val_images, val_labels], val_images),\n",
    "    callbacks=[reduce_lr,early_stopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab777d6a",
   "metadata": {},
   "source": [
    "### Saving the model weights after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8781ab42",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvae.save_weights('saved_model_weights.h5')  # Specify the file name to save the weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
