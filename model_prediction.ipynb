{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e9d529e",
   "metadata": {},
   "source": [
    "### Importing all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232cea2e-fcd1-4072-a224-edc610646b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, UpSampling2D, Flatten, Dense, Reshape, Lambda, Concatenate\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff564dc",
   "metadata": {},
   "source": [
    "### Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85b302b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Concatenate, Conv2D, MaxPooling2D, Flatten, Conv2DTranspose\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Updated Parameters\n",
    "img_height, img_width, img_channels = 256, 256, 1 #Input image dimensions\n",
    "latent_dim = (16, 16, 1) #Latent space dimensions\n",
    "num_classes = 9 #Number of classes\n",
    "\n",
    "class CVAE(tf.keras.Model):\n",
    "    def __init__(self, latent_dim, num_classes):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.encoder = self.build_encoder()\n",
    "        self.decoder = self.build_decoder()\n",
    "\n",
    "    def build_encoder(self):\n",
    "        input_img = Input(shape=(img_height, img_width, img_channels))\n",
    "        input_label = Input(shape=(self.num_classes,))\n",
    "        label_embedding = Dense(img_height * img_width)(input_label)\n",
    "        label_embedding = Reshape((img_height, img_width, 1))(label_embedding)\n",
    "\n",
    "        x = Concatenate()([input_img, label_embedding])\n",
    "        x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "        x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "        x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "        x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)\n",
    "        x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "        x = Flatten()(x)\n",
    "        mean = Dense(self.latent_dim[0] * self.latent_dim[1] * self.latent_dim[2])(x)\n",
    "        log_var = Dense(self.latent_dim[0] * self.latent_dim[1] * self.latent_dim[2])(x)\n",
    "        mean = Reshape(self.latent_dim)(mean)\n",
    "        log_var = Reshape(self.latent_dim)(log_var)\n",
    "        return Model([input_img, input_label], [mean, log_var], name='encoder')\n",
    "\n",
    "    def build_decoder(self):\n",
    "        latent_input = Input(shape=self.latent_dim)\n",
    "        input_label = Input(shape=(self.num_classes,))\n",
    "        label_embedding = Dense(self.latent_dim[0] * self.latent_dim[1] * self.latent_dim[2])(input_label)\n",
    "        label_embedding = Reshape(self.latent_dim)(label_embedding)\n",
    "\n",
    "        x = Concatenate()([latent_input, label_embedding])\n",
    "        x = Reshape((self.latent_dim[0] * self.latent_dim[1] * self.latent_dim[2] * 2,))(x)\n",
    "        x = Dense(16 * 16 * 512, activation='relu')(x)\n",
    "        x = Reshape((16, 16, 512))(x)\n",
    "        x = Conv2DTranspose(256, (3, 3), activation='relu', strides=2, padding='same')(x)\n",
    "        x = Conv2DTranspose(128, (3, 3), activation='relu', strides=2, padding='same')(x)\n",
    "        x = Conv2DTranspose(64, (3, 3), activation='relu', strides=2, padding='same')(x)\n",
    "        x = Conv2DTranspose(32, (3, 3), activation='relu', strides=2, padding='same')(x)\n",
    "        output_img = Conv2D(img_channels, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "        return Model([latent_input, input_label], output_img, name='decoder')\n",
    "\n",
    "    def sampling(self, args):\n",
    "        mean, log_var = args\n",
    "        epsilon = tf.random.normal(shape=tf.shape(mean), mean=0., stddev=1.)\n",
    "        return mean + tf.exp(log_var / 2) * epsilon\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_img, input_label = inputs\n",
    "        mean, log_var = self.encoder([input_img, input_label])\n",
    "        z = self.sampling([mean, log_var])\n",
    "        reconstructed = self.decoder([z, input_label])\n",
    "        reconstruction_loss = tf.reduce_mean(MeanSquaredError()(input_img, reconstructed))\n",
    "        kl_loss = -0.5 * tf.reduce_mean(1 + log_var - tf.square(mean) - tf.exp(log_var))\n",
    "        self.add_loss(reconstruction_loss + kl_loss)\n",
    "        return reconstructed\n",
    "\n",
    "cvae = CVAE(latent_dim, num_classes)\n",
    "cvae.compile(optimizer=Adam())\n",
    "\n",
    "cvae.encoder.summary()\n",
    "cvae.decoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152bf612",
   "metadata": {},
   "source": [
    "### Loading the saved model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2058b46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the model by running a dummy input through it\n",
    "dummy_image = tf.random.normal((1, 256, 256, 1))  # Dummy input for the image\n",
    "dummy_composition = tf.random.normal((1, 9))  # Dummy input for the composition\n",
    "_ = cvae([dummy_image, dummy_composition])  # Passing the dummy inputs to create variables\n",
    "\n",
    "# Loading the saved weights\n",
    "cvae.load_weights('saved_model_weights.h5')\n",
    "\n",
    "print(\"Model weights reloaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3def134-062b-41bc-881a-ed844e6b2b0d",
   "metadata": {},
   "source": [
    "### Performing Cubic interpolation to generate images of desired composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1889804a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import CubicSpline\n",
    "from utils import extract_c_avg, calculate_rounded_avg\n",
    "\n",
    "num_interpolations = 50\n",
    "target_rounded_avg = extract_c_avg('inter_comp.dat') # Extracting targeted composition value from inter_comp.dat file\n",
    "tolerance = 0.001\n",
    "desired_num_images = 500 # Number of images of target_rounded_avg to generate\n",
    "output_folder = f'{target_rounded_avg}_gen'\n",
    "label_file = f'interpolated_labels_{target_rounded_avg}.txt'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "gen_folder = output_folder\n",
    "\n",
    "def generate_images_for_rounded_avg(cvae, start_label, intermediate_label_2, intermediate_label_3, intermediate_label_4, intermediate_label_5, intermediate_label_6, intermediate_label_7, intermediate_label_8, end_label, target_rounded_avg, tolerance, desired_num_images):\n",
    "    generated_images_info = []\n",
    "\n",
    "    start_label = np.array(start_label)\n",
    "    intermediate_label_2 = np.array(intermediate_label_2)\n",
    "    intermediate_label_3 = np.array(intermediate_label_3)\n",
    "    intermediate_label_4 = np.array(intermediate_label_4)\n",
    "    intermediate_label_5 = np.array(intermediate_label_5)\n",
    "    intermediate_label_6 = np.array(intermediate_label_6)\n",
    "    intermediate_label_7 = np.array(intermediate_label_7)\n",
    "    intermediate_label_8 = np.array(intermediate_label_8)\n",
    "    end_label = np.array(end_label)\n",
    "\n",
    "    x_vals = [0.27, 0.31, 0.35, 0.39, 0.40, 0.42, 0.44, 0.46, 0.48]  # The positions of start and end points\n",
    "    y_vals = np.vstack([start_label, intermediate_label_2, intermediate_label_3, intermediate_label_4, intermediate_label_5, intermediate_label_6, intermediate_label_7, intermediate_label_8, end_label])\n",
    "    cubic_interp_func = CubicSpline(x_vals, y_vals, axis=0)\n",
    "    \n",
    "    if target_rounded_avg >= 0.27 and target_rounded_avg < 0.29:\n",
    "        start = 0.27\n",
    "        end = 0.29\n",
    "    elif target_rounded_avg >=0.29 and target_rounded_avg < 0.31:\n",
    "        start = 0.29\n",
    "        end = 0.31\n",
    "    elif target_rounded_avg == 0.31 :\n",
    "        start = 0.31\n",
    "        end = 0.33\n",
    "    elif target_rounded_avg > 0.31 and target_rounded_avg <=0.33:\n",
    "        start = 0.31\n",
    "        end = 0.33\n",
    "    elif target_rounded_avg >0.33 and target_rounded_avg < 0.35:\n",
    "        start = 0.33\n",
    "        end = 0.35\n",
    "    elif target_rounded_avg == 0.35 :\n",
    "        start = 0.35\n",
    "        end = 0.37\n",
    "    elif target_rounded_avg > 0.35 and target_rounded_avg <=0.37:       \n",
    "        start = 0.35\n",
    "        end = 0.37\n",
    "    elif target_rounded_avg >0.37 and target_rounded_avg < 0.39:\n",
    "        start = 0.37\n",
    "        end = 0.39\n",
    "    elif target_rounded_avg == 0.39 :\n",
    "        start = 0.39\n",
    "        end = 0.40\n",
    "    elif target_rounded_avg >0.39 and target_rounded_avg <0.40:\n",
    "        start = 0.39\n",
    "        end = 0.40\n",
    "    elif target_rounded_avg == 0.40:\n",
    "        start = 0.40\n",
    "        end = 0.41\n",
    "    elif target_rounded_avg == 0.41:\n",
    "        start = 0.40\n",
    "        end = 0.41\n",
    "    elif target_rounded_avg == 0.42:\n",
    "        start = 0.42\n",
    "        end = 0.43\n",
    "    elif target_rounded_avg == 0.43:\n",
    "        start = 0.42\n",
    "        end = 0.43\n",
    "    elif target_rounded_avg == 0.44:\n",
    "        start = 0.44\n",
    "        end = 0.45\n",
    "    elif target_rounded_avg == 0.45:\n",
    "        start = 0.44\n",
    "        end = 0.45\n",
    "    elif target_rounded_avg == 0.46:\n",
    "        start = 0.46\n",
    "        end = 0.47\n",
    "    elif target_rounded_avg == 0.47:\n",
    "        start = 0.47\n",
    "        end = 0.48\n",
    "    elif target_rounded_avg == 0.48:\n",
    "        start = 0.47\n",
    "        end = 0.48\n",
    "    \n",
    "    with open(label_file, 'w') as label_output:\n",
    "        while len(generated_images_info) < desired_num_images:\n",
    "            for alpha in np.linspace(start, end, num_interpolations):\n",
    "                if len(generated_images_info) >= desired_num_images:\n",
    "                    break\n",
    "\n",
    "                interpolated_label = cubic_interp_func(alpha)\n",
    "                random_latent_vector = tf.random.normal(shape=(1, *latent_dim))\n",
    "                interpolated_label_tensor = tf.convert_to_tensor(interpolated_label.reshape(1, -1), dtype=tf.float32)\n",
    "                generated_image = cvae.decoder([random_latent_vector, interpolated_label_tensor]).numpy().reshape(img_height, img_width)\n",
    "                generated_image_uint8 = (generated_image * 255).astype(np.uint8)\n",
    "                rounded_avg = calculate_rounded_avg(generated_image_uint8)\n",
    "\n",
    "                if abs(rounded_avg - target_rounded_avg) <= tolerance:\n",
    "                    generated_images_info.append((generated_image_uint8, rounded_avg))\n",
    "                    label_output.write(f\"image_{len(generated_images_info):03d}_interpolated_label_{interpolated_label}\\n\")\n",
    "\n",
    "    return generated_images_info\n",
    "\n",
    "start_label = [1, 0, 0, 0, 0, 0, 0, 0, 0]  # Start label\n",
    "intermediate_label_2 = [0, 1, 0, 0, 0, 0, 0, 0, 0]  # Intermediate label_2\n",
    "intermediate_label_3 = [0, 0, 1, 0, 0, 0, 0, 0, 0]  # Intermediate label_3\n",
    "intermediate_label_4 = [0, 0, 0, 1, 0, 0, 0, 0, 0]  # Intermediate label_4\n",
    "intermediate_label_5 = [0, 0, 0, 0, 1, 0, 0, 0, 0]  # Intermediate label_5\n",
    "intermediate_label_6 = [0, 0, 0, 0, 0, 1, 0, 0, 0]  # Intermediate label_6\n",
    "intermediate_label_7 = [0, 0, 0, 0, 0, 0, 1, 0, 0]  # Intermediate label_7\n",
    "intermediate_label_8 = [0, 0, 0, 0, 0, 0, 0, 1, 0]  # Intermediate label_8\n",
    "\n",
    "end_label = [0, 0, 0, 0, 0, 0, 0, 0, 1]  # End label\n",
    "\n",
    "generated_images_info = generate_images_for_rounded_avg(cvae, start_label, intermediate_label_2, intermediate_label_3, intermediate_label_4, intermediate_label_5, intermediate_label_6, intermediate_label_7, intermediate_label_8, end_label, target_rounded_avg, tolerance, desired_num_images)\n",
    "\n",
    "for i, (image, rounded_avg) in enumerate(generated_images_info):\n",
    "    filename = f\"image_{i+1:03d}_rounded_avg_{rounded_avg:.2f}.png\"\n",
    "    filepath = os.path.join(output_folder, filename)\n",
    "    cv2.imwrite(filepath, image)\n",
    "\n",
    "print(f\"Generated and saved {len(generated_images_info)} images with target rounded average {target_rounded_avg}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb63cc34",
   "metadata": {},
   "source": [
    "#### Ensuring that each interpolated lable lies in a single line in the .txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d570256",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import fix_file_format_and_replace\n",
    "\n",
    "input_file = label_file  \n",
    "fix_file_format_and_replace(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dbde40",
   "metadata": {},
   "source": [
    "### Obtaining filename and average area of white shapes with the largest, smallest and middle average area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40256ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = gen_folder\n",
    "\n",
    "image_info = []\n",
    "\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".png\"):\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        _, binary = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        total_area = sum(cv2.contourArea(contour) for contour in contours)\n",
    "        \n",
    "        total_shapes = len(contours)\n",
    "        \n",
    "        if total_shapes>0:\n",
    "            avg_area = total_area/total_shapes\n",
    "        else:\n",
    "            avg_area = 0\n",
    "\n",
    "        image_info.append((filename, avg_area))\n",
    "\n",
    "if image_info:\n",
    "    # Sort images by average area\n",
    "    image_info.sort(key=lambda x: x[1])\n",
    "\n",
    "    max_filename, max_area = image_info[-1]\n",
    "    min_filename, min_area = image_info[0]\n",
    "    mid_index = len(image_info) // 2\n",
    "    mid_filename, mid_area = image_info[mid_index]\n",
    "    \n",
    "    middle_image_name = mid_filename\n",
    "    middle_image_area = mid_area\n",
    "    \n",
    "    if target_rounded_avg < 0.29:\n",
    "        initial_image_name = min_filename\n",
    "        initial_image_area = min_area\n",
    "        \n",
    "    print(f\"Smallest_avg_area: {min_filename} - Average_area: {min_area}\")\n",
    "    print(f\"Middle_avg_area: {mid_filename} - Average_area: {mid_area}\")\n",
    "    print(f\"Largest_avg_area: {max_filename} - Average_area: {max_area}\")\n",
    "else:\n",
    "    print(\"No images found in the specified folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcedd95a",
   "metadata": {},
   "source": [
    "### Performing Similarity Search to find the image corresponding to initial timestep value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364e0d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import compute_ssim_filtered_image_info\n",
    "\n",
    "if target_rounded_avg >=0.27 and target_rounded_avg <0.29:\n",
    "    data = np.genfromtxt(f'dataset/comp{int((0.27)*100)}_350.dat', dtype=float)\n",
    "    \n",
    "if target_rounded_avg >=0.29 and target_rounded_avg <=0.31:\n",
    "    data = np.genfromtxt(f'datset/comp{int((0.31)*100)}_301.dat', dtype=float)\n",
    "    \n",
    "elif target_rounded_avg >0.31 and target_rounded_avg <=0.33:\n",
    "    data = np.genfromtxt(f'dataset/comp{int((0.31)*100)}_301.dat', dtype=float)\n",
    "        \n",
    "elif target_rounded_avg >0.33 and target_rounded_avg <=0.35:\n",
    "    data = np.genfromtxt(f'dataset/comp{int((0.35)*100)}_301.dat', dtype=float)\n",
    "    \n",
    "elif target_rounded_avg >0.35 and target_rounded_avg <=0.37:\n",
    "    data = np.genfromtxt(f'dataset/comp{int((0.35)*100)}_301.dat', dtype=float)\n",
    "    \n",
    "elif target_rounded_avg >0.37 and target_rounded_avg <=0.39:\n",
    "    data = np.genfromtxt(f'dataset/comp{int((0.39)*100)}_301.dat', dtype=float)\n",
    "\n",
    "elif target_rounded_avg == 0.40:\n",
    "    data = np.genfromtxt(f'dataset/comp{int((0.40)*100)}_301.dat', dtype=float)\n",
    "    \n",
    "elif target_rounded_avg == 0.41:\n",
    "    data = np.genfromtxt(f'dataset/comp{int((0.40)*100)}_301.dat', dtype=float)\n",
    "    \n",
    "elif target_rounded_avg ==0.42:\n",
    "    data = np.genfromtxt(f'dataset/comp{int((0.42)*100)}_301.dat', dtype=float)\n",
    "        \n",
    "elif target_rounded_avg ==0.43:\n",
    "    data = np.genfromtxt(f'dataset/comp{int((0.42)*100)}_301.dat', dtype=float)\n",
    "    \n",
    "elif target_rounded_avg == 0.44:\n",
    "    data = np.genfromtxt(f'dataset/comp{int((0.44)*100)}_301.dat', dtype=float)\n",
    "    \n",
    "elif target_rounded_avg ==0.45:\n",
    "    data = np.genfromtxt(f'dataset/comp{int((0.44)*100)}_301.dat', dtype=float)\n",
    "    \n",
    "elif target_rounded_avg ==0.46:\n",
    "    data = np.genfromtxt(f'dataset/comp{int((0.46)*100)}_301.dat', dtype=float)\n",
    "    \n",
    "elif target_rounded_avg ==0.47:\n",
    "    data = np.genfromtxt(f'dataset/comp{int((0.48)*100)}_375.dat', dtype=float)\n",
    "    \n",
    "elif target_rounded_avg ==0.48:\n",
    "    data = np.genfromtxt(f'dataset/comp{int((0.48)*100)}_301.dat', dtype=float)\n",
    "    \n",
    "\n",
    "avg = np.mean(data)\n",
    "data_normalized = 255 * (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "data_uint8 = data_normalized.astype(np.uint8)\n",
    "reference_image = Image.fromarray(data_uint8, mode='L')\n",
    "reference_image.save(f'initial_reference_image_for_{target_rounded_avg}.png')\n",
    "\n",
    "reference_image = cv2.imread(f'initial_reference_image_for_{target_rounded_avg}.png', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "folder_path = gen_folder\n",
    "\n",
    "image_info = compute_ssim_filtered_image_info(reference_image, gen_folder, threshold=70)\n",
    "\n",
    "if image_info:\n",
    "    min_filename, min_area = image_info[0]\n",
    "    initial_image_name = min_filename\n",
    "    initial_image_area = min_area\n",
    "    print(f\"Smallest_avg_area: {min_filename} - Average_area: {min_area}\")\n",
    "else:\n",
    "    print(\"No images with similarity â‰¥ 60% found in the specified folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9f6f51",
   "metadata": {},
   "source": [
    "### Performing Similarity Search to find the image corresponding to final timestep value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f379c11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import compute_ssim_filtered_image_info\n",
    "    \n",
    "if target_rounded_avg >=0.27 and target_rounded_avg <0.29:\n",
    "    data = np.genfromtxt(f'dataset/comp{int((0.27)*100)}_1000.dat', dtype=float)\n",
    "    \n",
    "elif target_rounded_avg >=0.29 and target_rounded_avg <=0.31:\n",
    "    data = np.genfromtxt(f'dataset/comp{int((0.31)*100)}_1000.dat', dtype=float)\n",
    "    \n",
    "elif target_rounded_avg >0.31 and target_rounded_avg <=0.33:\n",
    "    data = np.genfromtxt(f'dataset/comp{int((0.31)*100)}_1000.dat', dtype=float)\n",
    "        \n",
    "elif target_rounded_avg >0.33 and target_rounded_avg <=0.35:\n",
    "    data = np.genfromtxt(f'dataset/comp{int((0.35)*100)}_1000.dat', dtype=float)\n",
    "    \n",
    "elif target_rounded_avg >0.35 and target_rounded_avg <=0.37:\n",
    "    data = np.genfromtxt(f'dataset/comp{int((0.35)*100)}_1000.dat', dtype=float)\n",
    "    \n",
    "elif target_rounded_avg >0.37 and target_rounded_avg <=0.39:\n",
    "    data = np.genfromtxt(f'dataset/comp{int((0.39)*100)}_1000.dat', dtype=float)\n",
    "    \n",
    "elif target_rounded_avg == 0.40:\n",
    "    data = np.genfromtxt(f'dataset/comp{int((0.40)*100)}_1000.dat', dtype=float)\n",
    "    \n",
    "elif target_rounded_avg == 0.41:\n",
    "    data = np.genfromtxt(f'dataset/comp{int((0.40)*100)}_1000.dat', dtype=float)\n",
    "    \n",
    "elif target_rounded_avg ==0.42:\n",
    "    data = np.genfromtxt(f'dataset/comp{int((0.42)*100)}_1000.dat', dtype=float)\n",
    "        \n",
    "elif target_rounded_avg ==0.43:\n",
    "    data = np.genfromtxt(f'dataset/comp{int((0.42)*100)}_1000.dat', dtype=float)\n",
    "    \n",
    "elif target_rounded_avg == 0.44:\n",
    "    data = np.genfromtxt(f'dataset/comp{int((0.44)*100)}_1000.dat', dtype=float)\n",
    "    \n",
    "elif target_rounded_avg ==0.45:\n",
    "    data = np.genfromtxt(f'dataset/comp{int((0.44)*100)}_1000.dat', dtype=float)\n",
    "    \n",
    "elif target_rounded_avg ==0.46:\n",
    "    data = np.genfromtxt(f'dataset/comp{int((0.46)*100)}_1000.dat', dtype=float)\n",
    "    \n",
    "elif target_rounded_avg ==0.47:\n",
    "    data = np.genfromtxt(f'dataset/comp{int((0.48)*100)}_1000.dat', dtype=float)\n",
    "    \n",
    "elif target_rounded_avg ==0.48:\n",
    "    data = np.genfromtxt(f'dataset/comp{int((0.48)*100)}_1000.dat', dtype=float)\n",
    "\n",
    "avg = np.mean(data)\n",
    "data_normalized = 255 * (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "data_uint8 = data_normalized.astype(np.uint8)\n",
    "reference_image = Image.fromarray(data_uint8, mode='L')\n",
    "reference_image.save(f'final_reference_image_for_{target_rounded_avg}.png')\n",
    "\n",
    "reference_image = cv2.imread(f'final_reference_image_for_{target_rounded_avg}.png', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "folder_path = gen_folder\n",
    "\n",
    "# Compare with generated images\n",
    "image_info = compute_ssim_filtered_image_info(reference_image, gen_folder, threshold=60)\n",
    "\n",
    "# Extract image with max average area\n",
    "if image_info:\n",
    "    max_filename, max_area = image_info[-1]\n",
    "    final_image_name = max_filename\n",
    "    final_image_area = max_area\n",
    "    print(f\"Largest_avg_area: {max_filename} - Average_area: {max_area}\")\n",
    "else:\n",
    "    print(\"No images with similarity â‰¥ 60% found in the specified folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9a8f2f",
   "metadata": {},
   "source": [
    "### Obtaining the filename and average area of the image with the middle timestep value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082b8b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import calculate_average_area\n",
    "\n",
    "for filename in os.listdir(gen_folder):\n",
    "    if filename.endswith(\".png\"):\n",
    "        image_path = os.path.join(gen_folder, filename)\n",
    "        avg_area = calculate_average_area(image_path)\n",
    "        if(abs(avg_area - ((initial_image_area + final_image_area)/2))< 1):\n",
    "            print(f\"expected middle average area:{((initial_image_area + final_image_area)/2)}\")\n",
    "            print(f\"Middle Image_name: {filename}, Middle Average Area: {avg_area:.2f}\")\n",
    "            middle_image_name = filename\n",
    "            middle_image_area = avg_area\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3558bc58",
   "metadata": {},
   "source": [
    "### Obtaining the interpolated labels corresponding to the intial, middle and final timestep images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e9edc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_interpolated_label\n",
    "\n",
    "# Example usage\n",
    "image_filename_1 = initial_image_name\n",
    "image_filename_2 = middle_image_name\n",
    "image_filename_3 = final_image_name\n",
    "text_file_path = label_file  # Replacing with the actual path to your .txt file\n",
    "initial_interpolated_label = get_interpolated_label(image_filename_1, text_file_path)\n",
    "middle_interpolated_label = get_interpolated_label(image_filename_2, text_file_path)\n",
    "final_interpolated_label = get_interpolated_label(image_filename_3, text_file_path)\n",
    "print(f\"initial_label: {initial_interpolated_label}\")\n",
    "print(f\"middle_label: {middle_interpolated_label}\")\n",
    "print(f\"final_label: {final_interpolated_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba701ff",
   "metadata": {},
   "source": [
    "### Performing SLERP (Spherical Linear Interpolation) to obtain the consistent evolution between intial and middle timestep image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50089de6",
   "metadata": {},
   "source": [
    "### SLERP_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50205d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import generate_interpolated_images_slerp1\n",
    "\n",
    "image_folder = gen_folder # Path to folder containing generated images\n",
    "img1_path = os.path.join(image_folder, initial_image_name)\n",
    "img2_path = os.path.join(image_folder, middle_image_name)\n",
    "\n",
    "n_images = 500 # Number of images to generate\n",
    "\n",
    "slerp_middle_label, slerp_image_area = generate_interpolated_images_slerp1(cvae, img1_path, img2_path, n_images,\n",
    "                                                                           initial_image_area = initial_image_area, \n",
    "                                                                           middle_image_area = middle_image_area, target_rounded_avg=target_rounded_avg, \n",
    "                                                                           initial_interpolated_label=initial_interpolated_label,\n",
    "                                                                           middle_interpolated_label=middle_interpolated_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961a4d1a",
   "metadata": {},
   "source": [
    "### Extracting the interpolated label of the last generated image by SLERP in appropriate format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877359c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the slerp_middle_label\n",
    "cleaned_label = slerp_middle_label.strip(\"[]\")\n",
    "slerp_middle_interp_label =  \", \".join(cleaned_label.split())\n",
    "\n",
    "print(slerp_middle_interp_label)\n",
    "print(slerp_image_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7738854a",
   "metadata": {},
   "source": [
    "### Performing SLERP (Spherical Linear Interpolation) to obtain the consistent evolution between middle and final timestep image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cdae0b",
   "metadata": {},
   "source": [
    "### SLERP_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707b1469",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import generate_interpolated_images_slerp2\n",
    "\n",
    "middle_folder = f'{target_rounded_avg}_slerp_2_1'\n",
    "image_folder = gen_folder  # Path to folder with generated images\n",
    "\n",
    "# Set img1_path and img2_path\n",
    "img1_path = os.path.join(middle_folder, f\"generated_slerp_1_image_{n_images}.png\")\n",
    "\n",
    "img2_path = os.path.join(image_folder, final_image_name)\n",
    "\n",
    "generate_interpolated_images_slerp2(cvae, img1_path, img2_path, n_images, initial_image_area = slerp_image_area, final_image_area=final_image_area, \n",
    "                                    target_rounded_avg=target_rounded_avg, slerp_middle_interp_label = slerp_middle_interp_label, \n",
    "                                    final_interpolated_label= final_interpolated_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ccf1c3",
   "metadata": {},
   "source": [
    "### Moving images generated by SLERP_1 and SLERP_2 into a single folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4223226c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import move_images_to_overall\n",
    "\n",
    "folder_1 = f'{target_rounded_avg}_slerp_2_1'\n",
    "folder_2 = f'{target_rounded_avg}_slerp_2_1_2'\n",
    "overall_folder = f'{target_rounded_avg}_slerp_overall'\n",
    "\n",
    "move_images_to_overall(folder_1, folder_2, overall_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
